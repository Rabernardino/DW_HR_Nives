[2025-07-27T14:31:14.954+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: data_extraction_to_snowflake.processing_tables manual__2025-07-27T14:31:12.331206+00:00 map_index=2 [queued]>
[2025-07-27T14:31:14.961+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: data_extraction_to_snowflake.processing_tables manual__2025-07-27T14:31:12.331206+00:00 map_index=2 [queued]>
[2025-07-27T14:31:14.962+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-07-27T14:31:14.971+0000] {taskinstance.py:2191} INFO - Executing <Mapped(_PythonDecoratedOperator): processing_tables> on 2025-07-27 14:31:12.331206+00:00
[2025-07-27T14:31:14.975+0000] {standard_task_runner.py:60} INFO - Started process 383 to run task
[2025-07-27T14:31:14.977+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'data_extraction_to_snowflake', 'processing_tables', 'manual__2025-07-27T14:31:12.331206+00:00', '--job-id', '219', '--raw', '--subdir', 'DAGS_FOLDER/etl_transfer_to_stage.py', '--cfg-path', '/tmp/tmpoxrgef7y', '--map-index', '2']
[2025-07-27T14:31:14.979+0000] {standard_task_runner.py:88} INFO - Job 219: Subtask processing_tables
[2025-07-27T14:31:15.018+0000] {task_command.py:423} INFO - Running <TaskInstance: data_extraction_to_snowflake.processing_tables manual__2025-07-27T14:31:12.331206+00:00 map_index=2 [running]> on host ba480c39b9ec
[2025-07-27T14:31:15.098+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='data_extraction_to_snowflake' AIRFLOW_CTX_TASK_ID='processing_tables' AIRFLOW_CTX_EXECUTION_DATE='2025-07-27T14:31:12.331206+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-27T14:31:12.331206+00:00'
[2025-07-27T14:31:15.099+0000] {etl_transfer_to_stage.py:56} INFO - Starting transfer data from table shift_type_info
[2025-07-27T14:31:15.107+0000] {base.py:83} INFO - Using connection ID 'snowflake' for task execution.
[2025-07-27T14:31:15.108+0000] {connection.py:370} INFO - Snowflake Connector for Python Version: 3.6.0, Python Version: 3.8.18, Platform: Linux-6.12.5-linuxkit-aarch64-with-glibc2.34
[2025-07-27T14:31:15.109+0000] {connection.py:1171} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-07-27T14:31:15.976+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/dags/etl_transfer_to_stage.py", line 59, in processing_each_table
    with SnowflakeHook(snowflake_conn_id='snowflake').get_conn() as snow_conn:
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 276, in get_conn
    conn = connector.connect(**conn_config)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/__init__.py", line 54, in Connect
    return SnowflakeConnection(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/connection.py", line 413, in __init__
    self.connect(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/connection.py", line 703, in connect
    self.__open_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/connection.py", line 1006, in __open_connection
    self.authenticate_with_retry(self.auth_class)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/connection.py", line 1277, in authenticate_with_retry
    self._authenticate(auth_instance)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/connection.py", line 1305, in _authenticate
    auth.authenticate(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/auth/_auth.py", line 250, in authenticate
    ret = self._rest._post_request(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 729, in _post_request
    ret = self.fetch(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 838, in fetch
    ret = self._request_exec_wrapper(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 974, in _request_exec_wrapper
    raise e
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 879, in _request_exec_wrapper
    return_object = self._request_exec(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 1188, in _request_exec
    raise err
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 1130, in _request_exec
    raise_failed_request_error(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/network.py", line 238, in raise_failed_request_error
    Error.errorhandler_wrapper(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 290, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 348, in hand_to_other_handler
    connection.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 221, in default_errorhandler
    raise error_class(
snowflake.connector.errors.InterfaceError: 250003 (08001): None: 404 Not Found: post https://RABERNAR.snowflakecomputing.com:443/session/v1/login-request?request_id=16caffa1-6253-4de0-946a-56fc976552de&databaseName=HR_NIVES&schemaName=STAGE&warehouse=DW_HR_NIVES&roleName=&request_guid=86c4f213-7ee7-47c5-8dc8-241de84df529

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_transfer_to_stage.py", line 69, in processing_each_table
    raise AirflowException(f"Failed on collecting the max_id from table {table_name}: {e}")
airflow.exceptions.AirflowException: Failed on collecting the max_id from table shift_type_info: 250003 (08001): None: 404 Not Found: post https://RABERNAR.snowflakecomputing.com:443/session/v1/login-request?request_id=16caffa1-6253-4de0-946a-56fc976552de&databaseName=HR_NIVES&schemaName=STAGE&warehouse=DW_HR_NIVES&roleName=&request_guid=86c4f213-7ee7-47c5-8dc8-241de84df529
[2025-07-27T14:31:15.979+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=data_extraction_to_snowflake, task_id=processing_tables, map_index=2, execution_date=20250727T143112, start_date=20250727T143114, end_date=20250727T143115
[2025-07-27T14:31:15.987+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 219 for task processing_tables (Failed on collecting the max_id from table shift_type_info: 250003 (08001): None: 404 Not Found: post https://RABERNAR.snowflakecomputing.com:443/session/v1/login-request?request_id=16caffa1-6253-4de0-946a-56fc976552de&databaseName=HR_NIVES&schemaName=STAGE&warehouse=DW_HR_NIVES&roleName=&request_guid=86c4f213-7ee7-47c5-8dc8-241de84df529; 383)
[2025-07-27T14:31:16.023+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-07-27T14:31:16.034+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
