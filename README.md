# Projeto DW - Human Resource

## Objetivo

Este projeto demonstra a construÃ§Ã£o de um Data Warehouse para o setor de Recursos Humanos da empresa Nives, para isso utilizando um pipeline de dados construido com a stack:

* **Airflow**: OrquestraÃ§Ã£o e automaÃ§Ã£o do pipeline.
* **PostgreSQL**: Fonte de dados transacional (OLTP).
* **Snowflake**: Data warehouse na nuvem.
* **dbt (Data Build Tool)**: TransformaÃ§Ãµes de dados e modelagem do DW.

O foco Ã© extrair dados de um banco de dados PostgreSQL local de RH, carregÃ¡-los em uma Ã¡rea de staging no Snowflake e, em seguida, transformar e modelar.

---

## Arquitetura do Pipeline

A arquitetura do projeto segue um padrÃ£o de **ELT (Extract, Load, Transform)**, orquestrado pelo Airflow.

1.  **Extract & Load (EL)**: O Airflow extrai dados de tabelas no PostgreSQL (schema `HR_NIVES`) e os carrega diretamente para tabelas de staging no Snowflake.
2.  **Transform (T)**: O dbt entra em aÃ§Ã£o, transformando os dados brutos na camada de staging do Snowflake em um modelo de dados dimensional (camadas `bronze`, `silver` e `gold`).

### Diagrama da Arquitetura

![Snowflake](assets/snowflake.png)

![Dbt](assets/dbt_1.png)

![Dbt](assets/dbt_2.png)

---

## ğŸ“ Estrutura do Projeto

A organizaÃ§Ã£o do projeto segue as melhores prÃ¡ticas para pipelines de dados e desenvolvimento de software:
.
â”œâ”€â”€ .dbt/
â”‚   â”œâ”€â”€ dbt_project.yml     
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ source.yml          
â”‚   â””â”€â”€ 
â”œâ”€â”€ .airflow/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ etl_transfer_to_stage.py       
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore                  
â”œâ”€â”€ .python-version
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ README.md                   
â”œâ”€â”€ poetry.lock
â””â”€â”€ pyproject.toml


